{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Function to encode categorical data\n",
    "def encode_categorical_columns(df):\n",
    "    encoders = {}\n",
    "    encoded_columns = {}\n",
    "    \n",
    "    for column in df.select_dtypes(include=['object']).columns:\n",
    "        encoder = LabelEncoder()\n",
    "        df[column] = encoder.fit_transform(df[column])\n",
    "        encoders[column] = encoder\n",
    "        encoded_columns[column] = list(encoder.classes_)\n",
    "        \n",
    "        print(f\"Column '{column}' has been encoded. Classes: {list(encoder.classes_)}\")\n",
    "        \n",
    "    return df, encoders, encoded_columns\n",
    "\n",
    "# Function to plot the data\n",
    "def plot_data(df, plot_types):\n",
    "    plot_types = [pt.strip() for pt in plot_types.split(',')]\n",
    "    columns = df.columns\n",
    "    \n",
    "    for col1 in columns:\n",
    "        for col2 in columns:\n",
    "            if col1 != col2:\n",
    "                fig, axes = plt.subplots(nrows=1, ncols=len(plot_types), figsize=(15, 5))\n",
    "                fig.suptitle(f'{col1} vs {col2}')\n",
    "                \n",
    "                if len(plot_types) == 1:\n",
    "                    axes = [axes]\n",
    "                \n",
    "                for ax, plot_type in zip(axes, plot_types):\n",
    "                    if plot_type == 'bar':\n",
    "                        df.groupby([col1, col2]).size().unstack().plot(kind='bar', stacked=True, ax=ax)\n",
    "                    elif plot_type == 'box':\n",
    "                        df[[col1, col2]].plot(kind='box', ax=ax)\n",
    "                    elif plot_type == 'scatter':\n",
    "                        df.plot(kind='scatter', x=col1, y=col2, ax=ax)\n",
    "                    elif plot_type == 'line':\n",
    "                        df.plot(kind='line', x=col1, y=col2, ax=ax)\n",
    "                    elif plot_type == 'hist':\n",
    "                        df[[col1, col2]].plot(kind='hist', ax=ax)\n",
    "                    ax.set_title(f'{plot_type.capitalize()} plot')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "# Function to identify outliers\n",
    "def find_outliers(df):\n",
    "    outliers = {}\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers[col] = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)][col].tolist()\n",
    "    return outliers\n",
    "\n",
    "# Function to convert dataframe to sentences\n",
    "def convert_to_sentences(data):\n",
    "    sentences = []\n",
    "    for index, row in data.iterrows():\n",
    "        sentence = \", \".join([f\"{col} is {val}\" for col, val in row.items()])\n",
    "        sentences.append(sentence)\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "# Function to process text with CodeParrot\n",
    "def process_with_codeparrot(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot-small\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"codeparrot/codeparrot-small\")\n",
    "    \n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    outputs = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=100)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Function to process text with GPT-2\n",
    "def process_with_gpt2(text):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    \n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    outputs = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=100)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Function to perform detailed analysis and prompt engineering\n",
    "def detailed_analysis(df, codeparrot_text, gpt2_text):\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    stats = {\n",
    "        'mean': numeric_df.mean(),\n",
    "        'median': numeric_df.median(),\n",
    "        'mode': numeric_df.mode().iloc[0],\n",
    "        'std_dev': numeric_df.std(),\n",
    "        'variance': numeric_df.var(),\n",
    "        'skewness': numeric_df.skew(),\n",
    "        'kurtosis': numeric_df.kurt(),\n",
    "        'min': numeric_df.min(),\n",
    "        'max': numeric_df.max(),\n",
    "        'range': numeric_df.max() - numeric_df.min(),\n",
    "        '25%': numeric_df.quantile(0.25),\n",
    "        '50% (median)': numeric_df.quantile(0.50),\n",
    "        '75%': numeric_df.quantile(0.75),\n",
    "        'IQR': numeric_df.quantile(0.75) - numeric_df.quantile(0.25),\n",
    "        'correlation': numeric_df.corr()\n",
    "    }\n",
    "    \n",
    "    analysis_text = \"\\n\".join([f\"{key}:\\n{value}\\n\" for key, value in stats.items()])\n",
    "    outliers = find_outliers(df)\n",
    "    \n",
    "    prompt = (\n",
    "        f\"The dataset contains information related to [Brief Description of the Dataset's Domain]. \"\n",
    "        f\"Here is a detailed analytical summary of the data based on the following statistical analysis:\\n\\n{analysis_text}\\n\"\n",
    "        f\"Outliers:\\n{outliers}\\n\\n\"\n",
    "        f\"Please provide a detailed summary including:\\n\"\n",
    "        f\"- Structure of the dataset (columns and their meanings)\\n\"\n",
    "        f\"- Sample data points (5 rows for example)\\n\"\n",
    "        f\"- Key insights derived from the statistical analysis (relationships between variables, distributions, etc.)\\n\"\n",
    "        f\"- Any notable trends or patterns in the data\\n\"\n",
    "        f\"- Unusual or unexpected findings (anomalies)\\n\"\n",
    "        f\"- Actionable recommendations or potential areas for further investigation based on the analysis\"\n",
    "    )\n",
    "    \n",
    "    combined_text = codeparrot_text + \" \" + gpt2_text\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    \n",
    "    # Ensure pad token is set\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    inputs = tokenizer(prompt + combined_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=100)\n",
    "    detailed_description = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return analysis_text, detailed_description\n",
    "\n",
    "def main(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Encoding categorical columns\n",
    "    df, encoders, encoded_columns = encode_categorical_columns(df)\n",
    "    \n",
    "    # Asking user for plot types\n",
    "    plot_types = input(\"Enter the types of plots you want (e.g., histogram, scatter, line, bar, box), separated by commas: \")\n",
    "    \n",
    "    # Plotting the data\n",
    "    plot_data(df, plot_types)\n",
    "    \n",
    "    # Convert dataframe to sentences\n",
    "    text_data = convert_to_sentences(df)\n",
    "    \n",
    "    # Process text with CodeParrot\n",
    "    codeparrot_output = process_with_codeparrot(text_data)\n",
    "    print(\"CodeParrot Output:\")\n",
    "    print(codeparrot_output)\n",
    "    \n",
    "    # Process text with GPT-2\n",
    "    gpt2_output = process_with_gpt2(codeparrot_output)\n",
    "    print(\"GPT-2 Output:\")\n",
    "    print(gpt2_output)\n",
    "    \n",
    "    # Performing detailed analysis\n",
    "    analysis, description = detailed_analysis(df, codeparrot_output, gpt2_output)\n",
    "    print(\"Detailed Description:\\n\")\n",
    "    print(description)\n",
    "    print(\"\\nEncoded Columns:\\n\")\n",
    "    for col, classes in encoded_columns.items():\n",
    "        print(f\"{col}: {classes}\")\n",
    "\n",
    "# Example file path\n",
    "file_path = 'test_data.csv'\n",
    "main(file_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
